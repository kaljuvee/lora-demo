{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LoRA (Low-Rank Adaptation) Concepts Demo\n",
    "\n",
    "This notebook demonstrates the key concepts behind LoRA fine-tuning using practical examples with Ollama.\n",
    "\n",
    "## Learning Objectives\n",
    "1. Understand the mathematical foundation of LoRA\n",
    "2. Compare parameter efficiency of LoRA vs traditional fine-tuning\n",
    "3. Explore LoRA hyperparameters and their impact\n",
    "4. Implement a simple LoRA demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is LoRA?\n",
    "\n",
    "**Low-Rank Adaptation (LoRA)** is a parameter-efficient fine-tuning technique that allows us to adapt large pre-trained models to specific tasks without modifying all the original parameters.\n",
    "\n",
    "### Key Insight\n",
    "LoRA is based on the hypothesis that the weight updates during fine-tuning have a low \"intrinsic rank\". This means we can represent these updates using much smaller matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "print(\"Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Mathematical Foundation\n",
    "\n",
    "In traditional fine-tuning, we update the entire weight matrix:\n",
    "```\n",
    "W_new = W_original + ΔW\n",
    "```\n",
    "\n",
    "LoRA decomposes the update matrix ΔW into two smaller matrices:\n",
    "```\n",
    "ΔW = A × B\n",
    "```\n",
    "Where:\n",
    "- A has shape (d, r)\n",
    "- B has shape (r, d) \n",
    "- r << d (rank is much smaller than dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_lora_decomposition(d=1024, r=16):\n",
    "    \"\"\"\n",
    "    Demonstrate how LoRA decomposes a large matrix into smaller ones.\n",
    "    \"\"\"\n",
    "    print(f\"Original weight matrix size: {d} × {d} = {d*d:,} parameters\")\n",
    "    print(f\"LoRA rank: {r}\")\n",
    "    \n",
    "    # LoRA matrices\n",
    "    A_params = d * r\n",
    "    B_params = r * d\n",
    "    total_lora_params = A_params + B_params\n",
    "    \n",
    "    print(f\"\\nLoRA matrix A size: {d} × {r} = {A_params:,} parameters\")\n",
    "    print(f\"LoRA matrix B size: {r} × {d} = {B_params:,} parameters\")\n",
    "    print(f\"Total LoRA parameters: {total_lora_params:,}\")\n",
    "    \n",
    "    # Calculate efficiency\n",
    "    original_params = d * d\n",
    "    reduction = (1 - total_lora_params / original_params) * 100\n",
    "    \n",
    "    print(f\"\\nParameter reduction: {reduction:.2f}%\")\n",
    "    print(f\"Memory savings: {original_params / total_lora_params:.1f}x less parameters\")\n",
    "    \n",
    "    return {\n",
    "        'original_params': original_params,\n",
    "        'lora_params': total_lora_params,\n",
    "        'reduction_percent': reduction\n",
    "    }\n",
    "\n",
    "# Demonstrate with typical transformer dimensions\n",
    "result = demonstrate_lora_decomposition(d=4096, r=16)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visual Comparison: Traditional vs LoRA Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_parameter_comparison():\n",
    "    \"\"\"\n",
    "    Visualize parameter efficiency across different model sizes and ranks.\n",
    "    \"\"\"\n",
    "    model_sizes = [1e9, 7e9, 13e9, 30e9, 65e9]  # 1B, 7B, 13B, 30B, 65B parameters\n",
    "    model_names = ['1B', '7B', '13B', '30B', '65B']\n",
    "    ranks = [4, 8, 16, 32, 64]\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Plot 1: Parameter reduction by model size (rank=16)\n",
    "    rank = 16\n",
    "    reductions = []\n",
    "    for size in model_sizes:\n",
    "        # Estimate LoRA parameters (simplified calculation)\n",
    "        # Assuming ~32 layers, 4 matrices per layer, hidden_size proportional to sqrt(total_params)\n",
    "        hidden_size = int((size / 32 / 4) ** 0.5)\n",
    "        lora_params = 32 * 4 * (hidden_size * rank + rank * hidden_size)\n",
    "        reduction = (1 - lora_params / size) * 100\n",
    "        reductions.append(reduction)\n",
    "    \n",
    "    ax1.bar(model_names, reductions, color='skyblue', alpha=0.7)\n",
    "    ax1.set_title(f'Parameter Reduction by Model Size (rank={rank})')\n",
    "    ax1.set_ylabel('Reduction (%)')\n",
    "    ax1.set_xlabel('Model Size')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Parameter reduction by rank (7B model)\n",
    "    model_size = 7e9\n",
    "    hidden_size = 4096  # Typical for 7B models\n",
    "    reductions_by_rank = []\n",
    "    \n",
    "    for r in ranks:\n",
    "        lora_params = 32 * 4 * (hidden_size * r + r * hidden_size)\n",
    "        reduction = (1 - lora_params / model_size) * 100\n",
    "        reductions_by_rank.append(reduction)\n",
    "    \n",
    "    ax2.plot(ranks, reductions_by_rank, marker='o', linewidth=2, markersize=8, color='orange')\n",
    "    ax2.set_title('Parameter Reduction by LoRA Rank (7B Model)')\n",
    "    ax2.set_ylabel('Reduction (%)')\n",
    "    ax2.set_xlabel('LoRA Rank')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.set_xticks(ranks)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_parameter_comparison()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. LoRA Hyperparameters\n",
    "\n",
    "The key hyperparameters in LoRA are:\n",
    "\n",
    "1. **Rank (r)**: Controls the size of the adaptation matrices\n",
    "2. **Alpha (α)**: Scaling factor for the LoRA weights\n",
    "3. **Target Modules**: Which layers to apply LoRA to\n",
    "4. **Dropout**: Regularization for LoRA layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_lora_hyperparameters():\n",
    "    \"\"\"\n",
    "    Analyze the impact of different LoRA hyperparameters.\n",
    "    \"\"\"\n",
    "    print(\"LoRA Hyperparameter Analysis\\n\" + \"=\"*40)\n",
    "    \n",
    "    # Rank analysis\n",
    "    print(\"\\n1. RANK (r) - Controls adapter capacity:\")\n",
    "    ranks = [4, 8, 16, 32, 64]\n",
    "    hidden_size = 4096\n",
    "    \n",
    "    for r in ranks:\n",
    "        params_per_layer = 2 * hidden_size * r  # A and B matrices\n",
    "        print(f\"   Rank {r:2d}: {params_per_layer:,} params per adapted layer\")\n",
    "    \n",
    "    print(\"\\n   Guidelines:\")\n",
    "    print(\"   - Lower rank (4-8): Faster, less memory, may underfit\")\n",
    "    print(\"   - Higher rank (32-64): More capacity, may overfit\")\n",
    "    print(\"   - Sweet spot: 16-32 for most tasks\")\n",
    "    \n",
    "    # Alpha analysis\n",
    "    print(\"\\n2. ALPHA (α) - Scaling factor:\")\n",
    "    print(\"   - Controls the magnitude of LoRA updates\")\n",
    "    print(\"   - Effective learning rate = α / r\")\n",
    "    print(\"   - Common values: 16, 32, 64\")\n",
    "    print(\"   - Higher α = stronger adaptation\")\n",
    "    \n",
    "    # Target modules\n",
    "    print(\"\\n3. TARGET MODULES - Which layers to adapt:\")\n",
    "    modules = {\n",
    "        \"q_proj, v_proj\": \"Query and Value (minimal)\",\n",
    "        \"q_proj, k_proj, v_proj, o_proj\": \"All attention (common)\",\n",
    "        \"All linear layers\": \"Maximum adaptation (expensive)\"\n",
    "    }\n",
    "    \n",
    "    for module, description in modules.items():\n",
    "        print(f\"   - {module}: {description}\")\n",
    "    \n",
    "    return \"Analysis complete!\"\n",
    "\n",
    "analyze_lora_hyperparameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Practical Example: Creating a Custom Ollama Model\n",
    "\n",
    "Let's create a practical example of how to use LoRA concepts with Ollama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ollama_modelfile(model_name=\"lora-ml-tutor\", base_model=\"llama3.2:1b\"):\n",
    "    \"\"\"\n",
    "    Create an Ollama Modelfile that simulates a LoRA-tuned model.\n",
    "    \"\"\"\n",
    "    modelfile_content = f\"\"\"# {model_name.upper()} - LoRA Fine-tuned Model\n",
    "# Base model: {base_model}\n",
    "# Simulated LoRA adaptation for ML education\n",
    "\n",
    "FROM {base_model}\n",
    "\n",
    "# LoRA-inspired parameters\n",
    "PARAMETER temperature 0.7\n",
    "PARAMETER top_p 0.9\n",
    "PARAMETER top_k 40\n",
    "PARAMETER repeat_penalty 1.1\n",
    "\n",
    "# System prompt optimized for ML education (simulating LoRA adaptation)\n",
    "SYSTEM \\\"\\\"\\\"You are an expert machine learning tutor specialized in explaining complex ML concepts clearly and concisely. You:\n",
    "\n",
    "1. Break down complex topics into digestible parts\n",
    "2. Use analogies and examples to illustrate concepts\n",
    "3. Provide mathematical foundations when relevant\n",
    "4. Focus on practical applications and implementations\n",
    "5. Encourage hands-on learning and experimentation\n",
    "\n",
    "Always structure your responses with clear headings and examples.\\\"\\\"\\\"\n",
    "\n",
    "# Template for consistent formatting\n",
    "TEMPLATE \\\"\\\"\\\"{{{{ if .System }}}}<|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "{{{{ .System }}}}<|eot_id|>{{{{ end }}}}{{{{ if .Prompt }}}}<|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{{{{ .Prompt }}}}<|eot_id|>{{{{ end }}}}<|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "\\\"\\\"\\\"\n",
    "\"\"\"\n",
    "    \n",
    "    # Save to file\n",
    "    os.makedirs('../models', exist_ok=True)\n",
    "    modelfile_path = f'../models/Modelfile.{model_name}'\n",
    "    \n",
    "    with open(modelfile_path, 'w') as f:\n",
    "        f.write(modelfile_content)\n",
    "    \n",
    "    print(f\"Modelfile created: {modelfile_path}\")\n",
    "    print(\"\\nTo create the model in Ollama, run:\")\n",
    "    print(f\"ollama create {model_name} -f {modelfile_path}\")\n",
    "    \n",
    "    return modelfile_content\n",
    "\n",
    "# Create the modelfile\n",
    "modelfile = create_ollama_modelfile()\n",
    "print(\"\\nModelfile content:\")\n",
    "print(\"=\" * 50)\n",
    "print(modelfile[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Sample Training Data for LoRA\n",
    "\n",
    "Let's create a sample dataset that could be used for LoRA fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ml_training_dataset():\n",
    "    \"\"\"\n",
    "    Create a sample dataset for ML education fine-tuning.\n",
    "    \"\"\"\n",
    "    training_data = [\n",
    "        {\n",
    "            \"instruction\": \"Explain what LoRA (Low-Rank Adaptation) is and why it's useful.\",\n",
    "            \"input\": \"\",\n",
    "            \"output\": \"LoRA (Low-Rank Adaptation) is a parameter-efficient fine-tuning technique that allows us to adapt large pre-trained models to specific tasks without modifying all the original parameters. Instead of updating the entire weight matrix W, LoRA decomposes the weight update ΔW into two smaller matrices A and B, where ΔW = A × B. This approach reduces trainable parameters by 90%+ while maintaining model performance, making fine-tuning faster and more memory-efficient.\"\n",
    "        },\n",
    "        {\n",
    "            \"instruction\": \"What are the key hyperparameters in LoRA and how do they affect training?\",\n",
    "            \"input\": \"\",\n",
    "            \"output\": \"The key LoRA hyperparameters are: 1) Rank (r): Controls the size of adaptation matrices - lower ranks (4-8) are faster but may underfit, higher ranks (32-64) provide more capacity but may overfit. 2) Alpha (α): Scaling factor that controls the magnitude of LoRA updates, with common values being 16, 32, or 64. 3) Target modules: Determines which layers to adapt - typically attention layers (q_proj, k_proj, v_proj, o_proj) for optimal balance of performance and efficiency.\"\n",
    "        },\n",
    "        {\n",
    "            \"instruction\": \"Compare traditional fine-tuning with LoRA fine-tuning.\",\n",
    "            \"input\": \"\",\n",
    "            \"output\": \"Traditional fine-tuning updates all model parameters, requiring full model storage and high computational resources. LoRA fine-tuning freezes the original weights and only trains small adapter matrices, reducing trainable parameters by 90%+, memory usage by ~70%, and training time by 2-3x. LoRA also enables modular adaptation - you can switch between different task-specific adapters using the same base model, while traditional fine-tuning requires separate model copies for each task.\"\n",
    "        },\n",
    "        {\n",
    "            \"instruction\": \"How does the mathematical foundation of LoRA work?\",\n",
    "            \"input\": \"\",\n",
    "            \"output\": \"LoRA is based on the hypothesis that weight updates during fine-tuning have low intrinsic rank. Mathematically, instead of updating W_new = W_original + ΔW where ΔW is a full-rank matrix, LoRA decomposes ΔW = A × B where A has shape (d×r) and B has shape (r×d), with r << d. During training, only A and B are updated while W_original remains frozen. During inference, the matrices are merged: W_final = W_original + α × A × B, where α is a scaling factor.\"\n",
    "        },\n",
    "        {\n",
    "            \"instruction\": \"What are the practical benefits of using LoRA for model deployment?\",\n",
    "            \"input\": \"\",\n",
    "            \"output\": \"LoRA offers several deployment advantages: 1) Storage efficiency - you only need to store small adapter files (MBs) instead of full model copies (GBs) for each task. 2) Memory efficiency - reduced GPU memory requirements during training and inference. 3) Modularity - easily switch between different specialized adapters on the same base model. 4) Cost reduction - lower computational requirements mean reduced cloud costs. 5) Faster iteration - quicker training cycles enable rapid experimentation and deployment of task-specific models.\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Save dataset\n",
    "    os.makedirs('../data', exist_ok=True)\n",
    "    dataset_path = '../data/ml_education_dataset.json'\n",
    "    \n",
    "    with open(dataset_path, 'w') as f:\n",
    "        json.dump(training_data, f, indent=2)\n",
    "    \n",
    "    print(f\"Training dataset created: {dataset_path}\")\n",
    "    print(f\"Dataset contains {len(training_data)} training examples\")\n",
    "    \n",
    "    # Display first example\n",
    "    print(\"\\nSample training example:\")\n",
    "    print(\"=\" * 30)\n",
    "    example = training_data[0]\n",
    "    print(f\"Instruction: {example['instruction']}\")\n",
    "    print(f\"Output: {example['output'][:200]}...\")\n",
    "    \n",
    "    return training_data\n",
    "\n",
    "dataset = create_ml_training_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary and Next Steps\n",
    "\n",
    "In this notebook, we've covered:\n",
    "\n",
    "1. **LoRA Fundamentals**: Mathematical foundation and key concepts\n",
    "2. **Parameter Efficiency**: Dramatic reduction in trainable parameters\n",
    "3. **Hyperparameters**: Rank, alpha, and target modules\n",
    "4. **Practical Implementation**: Ollama Modelfile creation\n",
    "5. **Training Data**: Sample dataset for ML education\n",
    "\n",
    "### Next Steps:\n",
    "1. Run the actual LoRA training (see `02_lora_training.ipynb`)\n",
    "2. Deploy the fine-tuned model to Ollama\n",
    "3. Compare base model vs fine-tuned model performance\n",
    "4. Experiment with different hyperparameters\n",
    "\n",
    "### Key Takeaways:\n",
    "- LoRA enables efficient fine-tuning with 90%+ parameter reduction\n",
    "- Rank and alpha are the most important hyperparameters to tune\n",
    "- LoRA adapters are modular and can be easily swapped\n",
    "- The technique works well for domain-specific adaptations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🎉 LoRA Concepts Demo Complete!\")\n",
    "print(\"\\nFiles created:\")\n",
    "print(\"- ../models/Modelfile.lora-ml-tutor\")\n",
    "print(\"- ../data/ml_education_dataset.json\")\n",
    "print(\"\\nNext: Open 02_lora_training.ipynb for hands-on training!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
